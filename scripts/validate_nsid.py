#!/usr/bin/env python
"""Validate and analyze NSID files generated by MicroStack.

This script checks that HDF5 files are properly formatted according to the
NSID (N-Dimensional Spectroscopy and Imaging Data) specification, and can
perform basic analysis using sidpy/pycroscopy tools.

Usage:
    python scripts/validate_nsid.py path/to/file.h5
    python scripts/validate_nsid.py path/to/file.h5 --analyze
    python scripts/validate_nsid.py path/to/directory  # validates all .h5 files

Requirements:
    pip install sidpy pyNSID h5py matplotlib numpy
"""

import argparse
import sys
from pathlib import Path

import numpy as np

try:
    import h5py
    import sidpy
    import pyNSID
    import pyNSID.io.hdf_utils as pynsid_hdf

    DEPS_AVAILABLE = True
except ImportError as e:
    DEPS_AVAILABLE = False
    MISSING_DEP = str(e)

try:
    import matplotlib.pyplot as plt
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False

try:
    import pycroscopy as px
    from pycroscopy.learn import MatrixFactor
    PYCROSCOPY_AVAILABLE = True
except ImportError:
    PYCROSCOPY_AVAILABLE = False


def validate_nsid_file(filepath: Path, verbose: bool = True) -> dict:
    """Validate a single NSID file.

    Args:
        filepath: Path to HDF5 file
        verbose: Print detailed output

    Returns:
        Validation result dictionary
    """
    result = {
        "filepath": str(filepath),
        "valid": True,
        "datasets": [],
        "errors": [],
        "warnings": [],
    }

    if not filepath.exists():
        result["valid"] = False
        result["errors"].append(f"File not found: {filepath}")
        return result

    try:
        with h5py.File(filepath, "r") as h5f:
            # Check file-level attributes
            file_format = h5f.attrs.get("file_format", None)
            if file_format is None:
                result["warnings"].append("Missing 'file_format' attribute")
            elif file_format != "NSID":
                result["warnings"].append(f"file_format is '{file_format}', expected 'NSID'")

            software = h5f.attrs.get("software", "unknown")
            data_source = h5f.attrs.get("data_source", "unknown")

            if verbose:
                print(f"\nüìÅ File: {filepath.name}")
                print(f"   Software: {software}, Source: {data_source}")

            # Find all datasets
            def check_dataset(name, obj):
                if not isinstance(obj, h5py.Dataset):
                    return

                # Skip dimension scale datasets (they're ancillary)
                if name.endswith("_dim_0") or name.endswith("_dim_1") or name.endswith("_dim_2"):
                    return
                # Skip if it's a dimension scale (has only 1D)
                if "/x" in name or "/y" in name or "/z" in name:
                    if obj.ndim == 1:
                        return

                ds_info = {
                    "path": name,
                    "shape": obj.shape,
                    "dtype": str(obj.dtype),
                    "attributes": {},
                    "dimensions": [],
                    "issues": [],
                }

                # Check required NSID attributes
                required_attrs = ["title", "units", "quantity", "data_type"]
                for attr in required_attrs:
                    if attr in obj.attrs:
                        val = obj.attrs[attr]
                        ds_info["attributes"][attr] = (
                            val.decode() if isinstance(val, bytes) else val
                        )
                    else:
                        ds_info["issues"].append(f"Missing '{attr}' attribute")
                        result["warnings"].append(f"{name}: Missing '{attr}'")

                # Check dimension scales
                if obj.dims:
                    for i, dim in enumerate(obj.dims):
                        if dim.label:
                            ds_info["dimensions"].append({
                                "index": i,
                                "label": dim.label,
                                "has_scale": len(dim) > 0,
                            })
                        else:
                            ds_info["issues"].append(f"Dimension {i} has no label")

                # Validate data_type values
                valid_data_types = [
                    "image", "IMAGE", "image_stack", "IMAGE_STACK",
                    "spectrum", "SPECTRUM", "spectral_image", "SPECTRAL_IMAGE",
                    "image_4d", "IMAGE_4D", "unknown", "UNKNOWN", "line_plot"
                ]
                data_type = ds_info["attributes"].get("data_type", "")
                if data_type and data_type not in valid_data_types:
                    ds_info["issues"].append(f"Unusual data_type: '{data_type}'")

                result["datasets"].append(ds_info)

            h5f.visititems(check_dataset)

            if not result["datasets"]:
                result["warnings"].append("No main datasets found in file")

    except Exception as e:
        result["valid"] = False
        result["errors"].append(f"Failed to read file: {e}")

    # Overall validity
    result["valid"] = len(result["errors"]) == 0

    if verbose:
        print_validation_result(result)

    return result


def print_validation_result(result: dict):
    """Pretty-print validation results."""
    print(f"\n{'='*60}")
    print(f"Validation: {Path(result['filepath']).name}")
    print("=" * 60)

    if result["valid"]:
        print("‚úÖ File structure is valid")
    else:
        print("‚ùå File has errors")

    # Datasets
    print(f"\nüìä Datasets found: {len(result['datasets'])}")
    for ds in result["datasets"]:
        print(f"\n   {ds['path']}")
        print(f"      Shape: {ds['shape']}")
        print(f"      Type: {ds['attributes'].get('data_type', 'unknown')}")
        print(f"      Units: {ds['attributes'].get('units', 'unknown')}")

        if ds["dimensions"]:
            dim_labels = [d["label"] for d in ds["dimensions"]]
            print(f"      Dimensions: [{', '.join(dim_labels)}]")
        else:
            print("      Dimensions: (none attached)")

        if ds["issues"]:
            for issue in ds["issues"]:
                print(f"      ‚ö†Ô∏è  {issue}")

    # Warnings
    if result["warnings"]:
        print(f"\n‚ö†Ô∏è  Warnings ({len(result['warnings'])}):")
        for w in result["warnings"]:
            print(f"   - {w}")

    # Errors
    if result["errors"]:
        print(f"\n‚ùå Errors ({len(result['errors'])}):")
        for e in result["errors"]:
            print(f"   - {e}")


def try_read_with_pynsid(filepath: Path, verbose: bool = True) -> list:
    """Try to read datasets using pyNSID to verify compatibility.
    
    Uses pyNSID.io.hdf_utils.read_h5py_dataset() to read NSID datasets
    into sidpy.Dataset objects.
    
    Returns list of (path, sidpy.Dataset) tuples.
    """
    if verbose:
        print(f"\nüîç Reading with pyNSID.io.hdf_utils.read_h5py_dataset()...")

    datasets = []
    
    def find_and_read_datasets(h5_obj, path=""):
        """Recursively find and read NSID datasets from HDF5 structure."""
        for key in h5_obj.keys():
            obj = h5_obj[key]
            full_path = f"{path}/{key}" if path else key
            
            if isinstance(obj, h5py.Dataset):
                # Skip dimension scale datasets (1D arrays for x, y, z coordinates)
                if obj.ndim < 2:
                    continue
                    
                try:
                    dataset = pynsid_hdf.read_h5py_dataset(obj)
                    datasets.append((full_path, dataset))
                    if verbose:
                        print(f"   ‚úÖ {full_path}: Read successfully")
                        print(f"      sidpy.Dataset shape: {dataset.shape}")
                        print(f"      data_type: {dataset.data_type}")
                        print(f"      title: {dataset.title}")
                except Exception as e:
                    if verbose:
                        print(f"   ‚ö†Ô∏è  {full_path}: {e}")
            elif isinstance(obj, h5py.Group):
                find_and_read_datasets(obj, full_path)
    
    try:
        with h5py.File(filepath, "r") as h5f:
            find_and_read_datasets(h5f)
        return datasets
    except Exception as e:
        if verbose:
            print(f"   ‚ùå Failed: {e}")
        return []


def analyze_nsid_file(filepath: Path, output_dir: Path = None, verbose: bool = True):
    """Perform analysis on NSID file using pycroscopy.
    
    Demonstrates pycroscopy ecosystem compatibility by:
    1. Computing statistics
    2. FFT power spectrum analysis (for images)
    3. Matrix factorization SVD/NMF (for spectral images)
    4. Generating visualizations
    """
    if not MATPLOTLIB_AVAILABLE:
        print("‚ö†Ô∏è  matplotlib not available, skipping visualization")
        return
    
    if output_dir is None:
        output_dir = filepath.parent
    
    print(f"\nüî¨ Analyzing NSID file with pycroscopy...")
    print("=" * 60)
    
    if PYCROSCOPY_AVAILABLE:
        print(f"   Using pycroscopy v{px.__version__}")
    else:
        print("   ‚ö†Ô∏è  pycroscopy not available, using basic analysis")
    
    datasets = try_read_with_pynsid(filepath, verbose=False)
    
    if not datasets:
        print("   No datasets could be read for analysis")
        return
    
    for path, dataset in datasets:
        print(f"\nüìà Analyzing: {path}")
        print("-" * 40)
        
        # Get raw data
        data = np.array(dataset)
        
        # Basic statistics
        print(f"   Shape: {data.shape}")
        print(f"   Data type: {dataset.data_type}")
        print(f"   Units: {dataset.units}")
        print(f"\n   üìä Statistics:")
        print(f"      Min:    {np.min(data):.6g}")
        print(f"      Max:    {np.max(data):.6g}")
        print(f"      Mean:   {np.mean(data):.6g}")
        print(f"      Std:    {np.std(data):.6g}")
        print(f"      Median: {np.median(data):.6g}")
        
        # Data type specific analysis
        data_type = str(dataset.data_type).lower()
        
        if 'image' in data_type and data.ndim == 2:
            analyze_image_with_pycroscopy(dataset, data, filepath, path, output_dir, verbose)
            
        elif 'spectral_image' in data_type and data.ndim >= 3:
            analyze_spectral_image_with_pycroscopy(dataset, data, filepath, path, output_dir, verbose)
            
        elif 'spectrum' in data_type and data.ndim == 1:
            analyze_spectrum(dataset, data, filepath, path, output_dir, verbose)
        
        # Show dimension info
        print(f"\n   üìê Dimension Info:")
        for i, (name, dim) in enumerate(dataset._axes.items()):
            dim_vals = np.array(dim.values) if hasattr(dim, 'values') else None
            if dim_vals is not None and len(dim_vals) > 0:
                print(f"      [{i}] {dim.name}: {len(dim_vals)} points, "
                      f"range [{dim_vals[0]:.3g}, {dim_vals[-1]:.3g}] {dim.units}")
    
    print(f"\n‚úÖ Analysis complete!")


def analyze_image_with_pycroscopy(dataset, data, filepath, path, output_dir, verbose):
    """Analyze 2D image using pycroscopy tools."""
    print(f"\n   üñºÔ∏è  Image Analysis (pycroscopy):")
    
    # Compute histogram
    hist, bin_edges = np.histogram(data.flatten(), bins=50)
    peak_bin = bin_edges[np.argmax(hist)]
    print(f"      Histogram peak: {peak_bin:.6g}")
    
    # Dynamic range
    dynamic_range = np.max(data) - np.min(data)
    print(f"      Dynamic range: {dynamic_range:.6g}")
    
    # FFT Power Spectrum using pycroscopy
    power_spec = None
    if PYCROSCOPY_AVAILABLE:
        try:
            print(f"\n   üî¨ FFT Power Spectrum (pycroscopy.fft):")
            power_spec = px.fft.power_spectrum(dataset, smoothing=3)
            ps_data = np.array(power_spec)
            print(f"      Power spectrum shape: {ps_data.shape}")
            print(f"      Max power: {np.max(ps_data):.6g}")
            print(f"      ‚úÖ FFT analysis successful")
        except Exception as e:
            print(f"      ‚ö†Ô∏è  FFT failed: {e}")
    
    # Create visualization
    n_cols = 4 if power_spec is not None else 3
    fig, axes = plt.subplots(1, n_cols, figsize=(5*n_cols, 4))
    
    # Main image
    im = axes[0].imshow(data, cmap='viridis', origin='lower')
    axes[0].set_title(f'{dataset.title or "Image"}')
    axes[0].set_xlabel(f'x ({dataset._axes[1].units})')
    axes[0].set_ylabel(f'y ({dataset._axes[0].units})')
    plt.colorbar(im, ax=axes[0], label=dataset.units)
    
    # Histogram
    axes[1].hist(data.flatten(), bins=50, color='steelblue', edgecolor='black', alpha=0.7)
    axes[1].set_xlabel(f'Value ({dataset.units})')
    axes[1].set_ylabel('Count')
    axes[1].set_title('Intensity Histogram')
    axes[1].axvline(np.mean(data), color='red', linestyle='--', label=f'Mean: {np.mean(data):.3g}')
    axes[1].legend()
    
    # Line profile (middle row)
    mid_row = data.shape[0] // 2
    x_vals = np.array(dataset._axes[1].values) if hasattr(dataset._axes[1], 'values') else np.arange(data.shape[1])
    axes[2].plot(x_vals, data[mid_row, :], 'b-', linewidth=1.5)
    axes[2].set_xlabel(f'x ({dataset._axes[1].units})')
    axes[2].set_ylabel(f'Intensity ({dataset.units})')
    axes[2].set_title(f'Line Profile (row {mid_row})')
    axes[2].grid(True, alpha=0.3)
    
    # Power spectrum (if available)
    if power_spec is not None:
        ps_data = np.array(power_spec)
        im_ps = axes[3].imshow(np.log10(ps_data + 1), cmap='inferno', origin='lower')
        axes[3].set_title('FFT Power Spectrum (log)')
        axes[3].set_xlabel('kx')
        axes[3].set_ylabel('ky')
        plt.colorbar(im_ps, ax=axes[3], label='log(Power)')
    
    plt.tight_layout()
    
    # Save figure
    fig_path = output_dir / f"{filepath.stem}_{path.split('/')[-1]}_pycroscopy_analysis.png"
    plt.savefig(fig_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"\n   üíæ Saved analysis figure: {fig_path}")


def analyze_spectral_image_with_pycroscopy(dataset, data, filepath, path, output_dir, verbose):
    """Analyze spectral image using pycroscopy matrix factorization."""
    print(f"\n   üé® Spectral Image Analysis (pycroscopy):")
    print(f"      Original shape: {data.shape}")
    
    # Matrix Factorization using pycroscopy
    if PYCROSCOPY_AVAILABLE:
        try:
            print(f"\n   üî¨ Matrix Factorization (pycroscopy.learn.MatrixFactor):")
            
            # Try SVD decomposition
            n_components = min(4, min(data.shape))
            print(f"      Method: SVD, n_components={n_components}")
            
            mf = MatrixFactor(dataset, method='svd', n_components=n_components, return_fit=True)
            result = mf.do_fit()
            
            print(f"      ‚úÖ SVD decomposition successful")
            if hasattr(result, 'shape'):
                print(f"      Result shape: {result.shape}")
            else:
                print(f"      Result type: {type(result)}")
            
            # Also try NMF if data is non-negative
            if np.all(data >= 0):
                print(f"\n      Method: NMF (Non-negative Matrix Factorization)")
                mf_nmf = MatrixFactor(dataset, method='nmf', n_components=n_components, return_fit=True)
                result_nmf = mf_nmf.do_fit()
                print(f"      ‚úÖ NMF decomposition successful")
                if hasattr(result_nmf, 'shape'):
                    print(f"      Result shape: {result_nmf.shape}")
                else:
                    print(f"      Result type: {type(result_nmf)}")
            
        except Exception as e:
            print(f"      ‚ö†Ô∏è  Matrix factorization failed: {e}")
    
    # Create visualization - show slices of the spectral image
    n_spectral = data.shape[-1] if data.ndim == 3 else data.shape[0]
    n_show = min(4, n_spectral)
    
    fig, axes = plt.subplots(1, n_show + 1, figsize=(4*(n_show+1), 4))
    
    # Mean image
    mean_img = np.mean(data, axis=-1) if data.ndim == 3 else np.mean(data, axis=0)
    im = axes[0].imshow(mean_img, cmap='viridis', origin='lower')
    axes[0].set_title('Mean Image')
    plt.colorbar(im, ax=axes[0])
    
    # Show selected spectral slices
    indices = np.linspace(0, n_spectral-1, n_show, dtype=int)
    for i, idx in enumerate(indices):
        if data.ndim == 3:
            slice_data = data[:, :, idx]
        else:
            slice_data = data[idx, :, :]
        im = axes[i+1].imshow(slice_data, cmap='viridis', origin='lower')
        axes[i+1].set_title(f'Slice {idx}')
        plt.colorbar(im, ax=axes[i+1])
    
    plt.tight_layout()
    
    fig_path = output_dir / f"{filepath.stem}_{path.split('/')[-1]}_spectral_analysis.png"
    plt.savefig(fig_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"\n   üíæ Saved spectral analysis figure: {fig_path}")


def analyze_spectrum(dataset, data, filepath, path, output_dir, verbose):
    """Analyze 1D spectrum."""
    print(f"\n   üìâ Spectrum Analysis:")
    
    # Find peaks
    try:
        from scipy.signal import find_peaks
        peaks, properties = find_peaks(data, prominence=np.std(data)*0.5)
        print(f"      Peaks found: {len(peaks)}")
        
        # Get x-axis values
        if hasattr(dataset._axes[0], 'values'):
            x_vals = np.array(dataset._axes[0].values)
        else:
            x_vals = np.arange(len(data))
        
        if len(peaks) > 0:
            peak_positions = x_vals[peaks]
            print(f"      Peak positions: {peak_positions[:5]}...")
    except ImportError:
        peaks = []
        x_vals = np.arange(len(data))
        print("      ‚ö†Ô∏è  scipy not available for peak finding")
    
    # Create visualization
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.plot(x_vals, data, 'b-', linewidth=1.5, label='Data')
    if len(peaks) > 0:
        ax.plot(x_vals[peaks], data[peaks], 'ro', markersize=8, label='Peaks')
    ax.set_xlabel(f'{dataset._axes[0].name} ({dataset._axes[0].units})')
    ax.set_ylabel(f'{dataset.quantity} ({dataset.units})')
    ax.set_title(dataset.title or 'Spectrum')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    fig_path = output_dir / f"{filepath.stem}_{path.split('/')[-1]}_spectrum.png"
    plt.savefig(fig_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"\n   üíæ Saved spectrum figure: {fig_path}")


def main():
    parser = argparse.ArgumentParser(
        description="Validate and analyze NSID files generated by MicroStack"
    )
    parser.add_argument(
        "path",
        type=str,
        help="Path to HDF5 file or directory containing .h5 files",
    )
    parser.add_argument(
        "--quiet", "-q",
        action="store_true",
        help="Only print summary",
    )
    parser.add_argument(
        "--test-read",
        action="store_true",
        help="Test reading with pyNSID.io.hdf_utils.read_h5py_dataset()",
    )
    parser.add_argument(
        "--analyze",
        action="store_true",
        help="Perform analysis using sidpy (compute stats, generate plots)",
    )
    parser.add_argument(
        "--output-dir", "-o",
        type=str,
        default=None,
        help="Output directory for analysis plots (default: same as input file)",
    )
    args = parser.parse_args()

    if not DEPS_AVAILABLE:
        print(f"‚ùå Missing dependencies: {MISSING_DEP}")
        print("   Install with: pip install sidpy pyNSID h5py")
        sys.exit(1)

    path = Path(args.path)
    output_dir = Path(args.output_dir) if args.output_dir else None

    if path.is_file():
        files = [path]
    elif path.is_dir():
        files = list(path.rglob("*.h5"))
        if not files:
            print(f"No .h5 files found in {path}")
            sys.exit(1)
    else:
        print(f"Path not found: {path}")
        sys.exit(1)

    print(f"\nüî¨ MicroStack NSID Validator & Analyzer")
    print(f"   Processing {len(files)} file(s)...\n")

    all_valid = True
    for filepath in files:
        result = validate_nsid_file(filepath, verbose=not args.quiet)
        if not result["valid"]:
            all_valid = False

        if args.test_read:
            try_read_with_pynsid(filepath, verbose=not args.quiet)
        
        if args.analyze:
            analyze_nsid_file(filepath, output_dir=output_dir or filepath.parent, verbose=not args.quiet)

    # Summary
    print(f"\n{'='*60}")
    if all_valid:
        print(f"‚úÖ All {len(files)} file(s) are valid NSID format")
    else:
        print(f"‚ö†Ô∏è  Some files have issues")

    sys.exit(0 if all_valid else 1)


if __name__ == "__main__":
    main()
